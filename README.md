VoidForge AI – Gesture Controlled 3D Builder
Overview

VoidForge AI is an interactive web-based application that allows users to create and manipulate 3D objects using hand gestures detected through a webcam.
The system uses real-time gesture recognition to replace traditional mouse and keyboard controls, enabling a natural and immersive human–computer interaction experience.

The project demonstrates how Artificial Intelligence and computer vision can be applied to build intuitive interfaces for 3D design environments.

Features

Real-time hand gesture recognition using webcam

Control and creation of 3D objects without mouse/keyboard

Interactive 3D voxel building environment

Visual feedback while interacting with objects

Audio feedback for user actions

Works directly in a web browser

Technologies Used

JavaScript

HTML5 & CSS3

WebGL / Three.js (3D rendering)

MediaPipe / Hand Tracking (Computer Vision)

Web Audio API

System Requirements

A computer or laptop with webcam

Modern web browser (Google Chrome recommended)

Windows or Linux operating system

Installation & Setup

Download or clone the repository

git clone https://github.com/your-username/VoidForge-ai.git

Open the project folder

Run the project
Simply open:

index.html

(You can also use Live Server in VS Code for best performance.)

How to Use

Allow camera access when prompted.

Place your hand in front of the webcam.

Use gestures to interact with the environment:

Gesture	Action
Open Palm	Activate detection
Pinch / Select Gesture	Create block
Move Hand	Move cursor in 3D space
Gesture Change	Modify object / interaction

(Gestures may vary slightly depending on lighting and camera position.)

Project Modules

Camera Input Module – Captures real-time webcam video

Gesture Detection Module – Detects hand landmarks using computer vision

3D Rendering Module – Creates and displays 3D objects

Interaction Module – Maps gestures to actions

Audio Feedback Module – Provides sound feedback for user interaction

Applications

Virtual 3D modeling

Educational demonstrations

Human–computer interaction research

AR/VR interface prototyping

Accessible interfaces for physically challenged users

Limitations

Requires good lighting conditions

Webcam quality affects accuracy

Performance depends on browser and system hardware

Future Improvements

More gesture commands

Multi-hand interaction

Object saving and exporting

VR headset support

Improved gesture accuracy using trained models

Author

Mishal Sherif Moosa
B.Sc Artificial Intelligence & Machine Learning
Nehru Arts and Science College

License

This project is for educational and research purposes.
